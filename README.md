# DAPC: Efficient Prompt Compression with Debiased Attention for Long-Context Transformer Inference.

### Overview

Long-context inputs significantly increase the reasoning and memory cost of large language models. This repository builds on the LLMLingua family of methods to compress prompts on benchmark datasets such as LongBench, LongBench-E and InfiniteBench. It provides end-to-end scripts for prompt compression and evaluation, making it easy to reproduce results and extend to your own use cases.

Core components:
- `compress.py`: compresses prompts for benchmark datasets and outputs compressed JSONL files
- `eval.py`: reads model prediction results and performs unified evaluation and visualization for LongBench-style tasks
- `llmlingua/`: implementations and wrappers of various LLMLingua-based compressors

If you care about long-context efficiency, on-prem deployment, or how different compression schemes affect model performance, this project serves as a ready-to-use and easy-to-customize baseline system.

### Highlighted Features

- Multiple prompt compression strategies: LLMLingua, LLMLingua-2, LongLLMLingua, and DAC, EHPC, KVzip-PC, LRP-QA, Attn, PPL and more
- Support for both query-agnostic and query-aware settings: compress pure context only, or context conditioned on tasks/questions
- Prompt templates and length configurations for LongBench / LongBench-E / InfiniteBench are pre-tuned to reproduce benchmark results out of the box
- Detailed evaluation and visualization scripts that aggregate scores per task, per category, and overall, and export to Excel
- Easy side-by-side comparison across compression methods, compression ratios and compressor model sizes

### Environment

Recommended environment:
- Python 3.9+

```bash
pip install -r requirements.txt
```

### Project Structure

```text
.
├── llmlingua/
│   ├── llmlingua.py   # llmlingua, llmlingua-2, longllmlingua
│   ├── dac.py         # attn, ppl, dac
│   ├── ehpc.py        # dapc, ehpc, kvzip
│   └── lrp.py         # lrp, rollout
├── consts.py
├── compress.py
├── compress.sh        # compress script
├── model_openai.py    # vllm wrapper
├── pred_vllm.py  
├── run.sh             # inference script
├── metrics.py
├── eval.py            # eval results
├── requirements.txt
└── README.md
```

### Compressor Implementations: `llmlingua/`

The `longbench/llmlingua` directory contains unified wrappers around different compression strategies. Main entry points include:

- `llmlingua/__init__.py`: exports `PromptCompressor`, `DACPromptCompressor`, `EHPCPromptCompressor`, `LRPPromptCompressor`, etc.
- `llmlingua/llmlingua.py`: core LLMLingua implementation
- `llmlingua/dac.py`, `llmlingua/ehpc.py`, `llmlingua/lrp.py`, etc.: specific compressor implementations

In `longbench/compress.py`, `CompressorFactory` maps method names to concrete compressor classes:
- `llmlingua`, `llmlingua-2`, `longllmlingua` → `PromptCompressor`
- `dac`, `attn`, `ppl` → `DACPromptCompressor`
- `ehpc`, `kvzip`, `p-contrast-qa`, `rollout-qa` → `EHPCPromptCompressor`
- `lrp-qa` → `LRPPromptCompressor`

To add a new compression method:
1. Implement a compressor class in `llmlingua/`
2. Export it in `__init__.py`
3. Register the method name in `CompressorFactory`

The new method will then plug into the existing data-processing and evaluation pipeline naturally.


### Compression Script: `compress.py`

This script performs prompt compression for the specified benchmark dataset and saves results as JSONL files. The main entry point is `run_compress`.

Common arguments:
- `--method`: compression method name, e.g. `kvzip`, `ehpc`, `dac`, `p-contrast-qa`, `rollout-qa`
- `--model`: base model path used by the compressor, e.g. `/data/hf/Qwen/Qwen2.5-7B-Instruct`
- `--chunk_size`: chunk size for long documents
- `--contrast_alpha`: mixing coefficient for contrastive-style methods
- `--rollout_m`: rollout steps
- `--target_token`: target number of tokens after compression
- `--compress_rate`: compression ratio (mutually exclusive with `target_token`)
- `--bench`: benchmark name, supports `longbench` and `longbench-e`
- `--dataset`: dataset name, e.g. `narrativeqa`, `qasper`
- `--data_dir`: root directory of datasets
- `--save_dir`: directory to store compressed results

Outputs are saved to:

```text
{model_name}_compressed/{bench}/{dataset}_{method_str}.jsonl
```

`method_str` is generated by `consts.get_method_para` and encodes key configuration such as method, `chunk_size` and `contrast_alpha`.

Example: compress `narrativeqa` on LongBench with KVzip-PC:

```bash
python compress.py \
  --method kvzip \
  --model /data/hf/Qwen/Qwen2.5-7B-Instruct \
  --bench longbench \
  --dataset narrativeqa \
  --target_token 2000 \
  --chunk_size 1024 \
  --contrast_alpha 1.0 \
  --rollout_m 1
```

The script automatically skips already-compressed files that contain scores, and cleans up missing or corrupted files.

### Evaluation Script: `eval.py`

The evaluation script reads JSONL prediction files, scores each task, aggregates by task category and overall, and can optionally export both detailed and summary Excel reports.

Evaluation workflow:
1. Iterate over experiment folders under `{compress_model_name}_pred/`, e.g.  
   `longbench_Qwen2.5-7B-Instruct_kvzip_cs1024_ca1.0_t2000_chunk`
2. Load prediction JSONL files for each dataset in the folder
3. Call the corresponding metric function in `metrics.py` to compute F1, ROUGE, classification accuracy, retrieval recall, code similarity, etc.
4. Average scores by task category (Single-Document QA, Multi-Document QA, Summarization, etc.) and overall
5. Save detailed and aggregated results to the `excel_log/` directory

You can simply run:

```bash
python eval.py
```

At the end of the script in the `if __name__ == "__main__":` block, you can modify:
- `filter_dict`: filter by benchmark, model size, method and hyperparameters
- `bench_type`: choose `longbench` or `longbench-e`
- `target`: evaluate only a specific dataset, or `None` for all

The script also exposes several visualization utilities:
- `view_different_ratio`: compare performance across different compression ratios
- `view_different_compressor`: compare different compressor model sizes and methods
- `view_pred`: quickly inspect raw predictions for a given experiment configuration

Figures are saved under `exp_pics/` and can be used directly for publication-quality plots.


### Usage and Extensions

- Use this repository as a baseline to compare different compression algorithms on LongBench
- Quickly reproduce LLMLingua-style methods locally or on servers, combined with open-source models such as Qwen2.5
- Design your own prompt templates and compression strategies while reusing `CompressorFactory` and the evaluation scripts
- Take this repository as a starting point for long-context system optimization and measure real-world gains in throughput and cost

If this repository is helpful to your research or projects, consider giving it a star on GitHub and citing it in your papers or reports. Feedback and contributions are welcome.
