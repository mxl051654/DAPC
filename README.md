# DAPC: Efficient Prompt Compression with Debiased Attention for Long-Context Transformer Inference

## Language
- [English](#English Version)
- [中文](#中文说明)

## English Version

### Overview

Long-context inputs significantly increase the reasoning and memory cost of large language models. This repository builds on the LLMLingua family of methods to compress prompts on benchmark datasets such as LongBench, LongBench-E and InfiniteBench. It provides end-to-end scripts for prompt compression and evaluation, making it easy to reproduce results and extend to your own use cases.

Core components:
- `compress.py`: compresses prompts for benchmark datasets and outputs compressed JSONL files
- `eval.py`: reads model prediction results and performs unified evaluation and visualization for LongBench-style tasks
- `llmlingua/`: implementations and wrappers of various LLMLingua-based compressors

If you care about long-context efficiency, on-prem deployment, or how different compression schemes affect model performance, this project serves as a ready-to-use and easy-to-customize baseline system.

### Highlighted Features

- Multiple prompt compression strategies: LLMLingua, LLMLingua-2, LongLLMLingua, and DAC, EHPC, KVzip-PC, LRP-QA, Attn, PPL and more
- Support for both query-agnostic and query-aware settings: compress pure context only, or context conditioned on tasks/questions
- Prompt templates and length configurations for LongBench / LongBench-E / InfiniteBench are pre-tuned to reproduce benchmark results out of the box
- Detailed evaluation and visualization scripts that aggregate scores per task, per category, and overall, and export to Excel
- Easy side-by-side comparison across compression methods, compression ratios and compressor model sizes

### Environment

Recommended environment:
- Python 3.9+

```bash
pip install -r requirements.txt
```

### Compression Script: `longbench/compress.py`

This script performs prompt compression for the specified benchmark dataset and saves results as JSONL files. The main entry point is `run_compress`.

Common arguments:
- `--method`: compression method name, e.g. `kvzip`, `ehpc`, `dac`, `p-contrast-qa`, `rollout-qa`
- `--model`: base model path used by the compressor, e.g. `/data/hf/Qwen/Qwen2.5-7B-Instruct`
- `--chunk_size`: chunk size for long documents
- `--contrast_alpha`: mixing coefficient for contrastive-style methods
- `--rollout_m`: rollout steps
- `--target_token`: target number of tokens after compression
- `--compress_rate`: compression ratio (mutually exclusive with `target_token`)
- `--bench`: benchmark name, supports `longbench` and `longbench-e`
- `--dataset`: dataset name, e.g. `narrativeqa`, `qasper`
- `--data_dir`: root directory of datasets
- `--save_dir`: directory to store compressed results

Outputs are saved to:

```text
{model_name}_compressed/{bench}/{dataset}_{method_str}.jsonl
```

`method_str` is generated by `consts.get_method_para` and encodes key configuration such as method, `chunk_size` and `contrast_alpha`.

Example: compress `narrativeqa` on LongBench with KVzip-PC:

```bash
python longbench/compress.py \
  --method kvzip \
  --model /data/hf/Qwen/Qwen2.5-7B-Instruct \
  --bench longbench \
  --dataset narrativeqa \
  --target_token 2000 \
  --chunk_size 1024 \
  --contrast_alpha 1.0 \
  --rollout_m 1
```

The script automatically skips already-compressed files that contain scores, and cleans up missing or corrupted files.



### Evaluation Script: `longbench/eval.py`

The evaluation script reads JSONL prediction files, scores each task, aggregates by task category and overall, and can optionally export both detailed and summary Excel reports.

Evaluation workflow:
1. Iterate over experiment folders under `{compress_model_name}_pred/`, e.g.  
   `longbench_Qwen2.5-7B-Instruct_kvzip_cs1024_ca1.0_t2000_chunk`
2. Load prediction JSONL files for each dataset in the folder
3. Call the corresponding metric function in `metrics.py` to compute F1, ROUGE, classification accuracy, retrieval recall, code similarity, etc.
4. Average scores by task category (Single-Document QA, Multi-Document QA, Summarization, etc.) and overall
5. Save detailed and aggregated results to the `excel_log/` directory

You can simply run:

```bash
python longbench/eval.py
```

At the end of the script in the `if __name__ == "__main__":` block, you can modify:
- `filter_dict`: filter by benchmark, model size, method and hyperparameters
- `bench_type`: choose `longbench` or `longbench-e`
- `target`: evaluate only a specific dataset, or `None` for all

The script also exposes several visualization utilities:
- `view_different_ratio`: compare performance across different compression ratios
- `view_different_compressor`: compare different compressor model sizes and methods
- `view_pred`: quickly inspect raw predictions for a given experiment configuration

Figures are saved under `exp_pics/` and can be used directly for publication-quality plots.

### Compressor Implementations: `longbench/llmlingua/`

The `longbench/llmlingua` directory contains unified wrappers around different compression strategies. Main entry points include:

- `llmlingua/__init__.py`: exports `PromptCompressor`, `DACPromptCompressor`, `EHPCPromptCompressor`, `LRPPromptCompressor`, etc.
- `llmlingua/llmlingua.py`: core LLMLingua implementation
- `llmlingua/dac.py`, `llmlingua/ehpc.py`, `llmlingua/lrp.py`, etc.: specific compressor implementations

In `longbench/compress.py`, `CompressorFactory` maps method names to concrete compressor classes:
- `llmlingua`, `llmlingua-2`, `longllmlingua` → `PromptCompressor`
- `dac`, `attn`, `ppl` → `DACPromptCompressor`
- `ehpc`, `kvzip`, `p-contrast-qa`, `rollout-qa` → `EHPCPromptCompressor`
- `lrp-qa` → `LRPPromptCompressor`

To add a new compression method:
1. Implement a compressor class in `llmlingua/`
2. Export it in `__init__.py`
3. Register the method name in `CompressorFactory`

The new method will then plug into the existing data-processing and evaluation pipeline naturally.

### Project Structure

```text
.
├── llmlingua/
│   ├── llmlingua.py   # llmlingua, llmlingua-2, longllmlingua
│   ├── dac.py         # attn, ppl, dac
│   ├── ehpc.py        # dapc, ehpc, kvzip
│   └── lrp.py         # lrp, rollout
├── consts.py
├── compress.py
├── compress.sh        # compress script
├── model_openai.py    # vllm wrapper
├── pred_vllm.py  
├── run.sh             # inference script
├── metrics.py
├── eval.py            # eval results
├── requirements.txt
└── README.md
```


### Usage and Extensions

- Use this repository as a baseline to compare different compression algorithms on LongBench
- Quickly reproduce LLMLingua-style methods locally or on servers, combined with open-source models such as Qwen2.5
- Design your own prompt templates and compression strategies while reusing `CompressorFactory` and the evaluation scripts
- Take this repository as a starting point for long-context system optimization and measure real-world gains in throughput and cost

If this repository is helpful to your research or projects, consider giving it a star on GitHub and citing it in your papers or reports. Feedback and contributions are welcome.

---

## 中文说明

### 项目简介

长上下文带来的推理与记忆开销，是当前大模型在真实应用中面临的关键瓶颈之一。本仓库基于 LLMLingua 系列方法，对 LongBench / LongBench-E / InfiniteBench 等基准数据集上的提示词进行压缩，并提供完整的压缩与评测脚本，便于快速复现与进一步扩展。

核心组件包括：
- `compress.py`：对基准数据集进行提示压缩，生成压缩后的 JSONL 文件
- `eval.py`：读取模型预测结果，对 LongBench 系列任务进行统一评测与可视化
- `llmlingua/`：基于 LLMLingua 的多种压缩器实现与封装

如果你在研究或工程中关注长上下文推理效率、本地部署成本，以及不同压缩方案对性能的影响，本项目可以作为一个开箱即用、易于改造的基线系统。

### 特性亮点

- 覆盖多种提示压缩策略：包含 LLMLingua、LLMLingua-2、LongLLMLingua，以及 DAC、EHPC、KVzip-PC、LRP-QA、Attn、PPL 等方法
- 同时支持 query-agnostic 与 query-aware 场景：既可以对纯上下文进行压缩，也可以结合任务与问题进行任务感知压缩
- 针对 LongBench / LongBench-E / InfiniteBench 的 prompt 模板与长度配置已调好，直接运行即可复现基准结果
- 提供详细的评测脚本与可视化工具，自动汇总各任务、各大类以及整体平均分，并导出为 Excel 表格
- 支持对不同压缩方法、压缩率以及压缩模型规模进行横向比较，辅助系统性分析

### 环境依赖

```bash
pip install -r requirements.txt
``` 

### 压缩脚本：`longbench/compress.py`

该脚本负责对指定基准数据集进行提示压缩，并将结果保存为 JSONL 文件。核心入口函数为 `run_compress`。

常用参数：
- `--method`：压缩方法名称，例如 `kvzip`、`ehpc`、`dac`、`p-contrast-qa`、`rollout-qa` 等
- `--model`：压缩器使用的底层模型路径，例如 `/data/hf/Qwen/Qwen2.5-7B-Instruct`
- `--chunk_size`：长文本切块大小
- `--contrast_alpha`：对比学习类方法的融合系数
- `--rollout_m`：rollout 步数
- `--target_token`：压缩后目标 token 数
- `--compress_rate`：压缩率（与 `target_token` 二选一）
- `--bench`：基准名称，支持 `longbench`、`longbench-e`
- `--dataset`：具体数据集名称，例如 `narrativeqa`、`qasper` 等
- `--data_dir`：数据根目录
- `--save_dir`：压缩结果保存目录

脚本运行时会自动将结果保存在：

```text
{model_name}_compressed/{bench}/{dataset}_{method_str}.jsonl
```

其中 `method_str` 由 `consts.get_method_para` 生成，包含方法名、`chunk_size`、`contrast_alpha` 等关键信息。

示例：使用 KVzip-PC 方法压缩 LongBench 上的 `narrativeqa`：

```bash
python longbench/compress.py \
  --method kvzip \
  --model /data/hf/Qwen/Qwen2.5-7B-Instruct \
  --bench longbench \
  --dataset narrativeqa \
  --target_token 2000 \
  --chunk_size 1024 \
  --contrast_alpha 1.0 \
  --rollout_m 1
```

脚本会自动跳过已压缩且包含评分信息的数据，并对缺失或损坏的文件进行清理。


### 评测脚本：`longbench/eval.py`

评测脚本负责读取推理结果 JSONL 文件，对不同任务进行打分，并按任务类别和整体平均分进行汇总，可选择导出为详细和摘要版 Excel 表。

评测流程概览：
1. 从 `{compress_model_name}_pred/` 目录中遍历实验文件夹，例如：  
   `longbench_Qwen2.5-7B-Instruct_kvzip_cs1024_ca1.0_t2000_chunk`
2. 在对应子目录下读取各数据集的预测结果 JSONL 文件
3. 按数据集名称调用对应的指标函数（见 `metrics.py`），计算 F1、ROUGE、分类准确率、检索召回率、代码相似度等
4. 按任务类别（Single-Document QA、Multi-Document QA、Summarization 等）以及整体任务求平均
5. 将详细结果和汇总结果分别保存到 `excel_log/` 目录下

你可以直接运行：

```bash
python longbench/eval.py
```

在脚本末尾的 `if __name__ == "__main__":` 部分，可以修改：
- `filter_dict`：筛选特定基准、模型规模、方法和超参数
- `bench_type`：选择 `longbench` 或 `longbench-e`
- `target`：只评测某一个数据集，默认为 `None` 表示全量评测

脚本还提供若干可视化函数：
- `view_different_ratio`：对比不同压缩率下的性能曲线
- `view_different_compressor`：对比不同压缩模型规模和压缩方法的表现
- `view_pred`：快速查看某个实验配置下的原始预测文本

图像会保存到 `exp_pics/` 目录下，支持论文绘图级别的精度与美观度设置。

### 压缩器实现：`longbench/llmlingua/`

`longbench/llmlingua` 目录包含针对不同压缩策略的统一接口封装，主要入口包括：

- `llmlingua/__init__.py`：导出 `PromptCompressor`、`DACPromptCompressor`、`EHPCPromptCompressor`、`LRPPromptCompressor` 等
- `llmlingua/llmlingua.py`：LLMLingua 基础实现
- `llmlingua/dac.py`、`llmlingua/ehpc.py`、`llmlingua/lrp.py` 等：对应的具体压缩器实现

在 `longbench/compress.py` 中，通过 `CompressorFactory` 将方法名映射到具体压缩器：
- `llmlingua`、`llmlingua-2`、`longllmlingua` 对应 `PromptCompressor`
- `dac`、`attn`、`ppl` 对应 `DACPromptCompressor`
- `ehpc`、`kvzip`、`p-contrast-qa`、`rollout-qa` 等对应 `EHPCPromptCompressor`
- `lrp-qa` 对应 `LRPPromptCompressor`

如果你希望加入新的压缩方法，只需要：
1. 在 `llmlingua/` 中实现新的压缩器类
2. 在 `__init__.py` 中导出该类
3. 在 `CompressorFactory` 中添加方法名与该类的映射逻辑

即可无缝接入现有的数据处理与评测流程。

### 项目结构示意

```text
.
├── llmlingua/
│   ├── llmlingua.py   # llmlingua, llmlingua-2, longllmlingua
│   ├── dac.py         # attn, ppl, dac
│   ├── ehpc.py        # dapc, ehpc, kvzip
│   └── lrp.py         # lrp, rollout
├── consts.py
├── compress.py
├── compress.sh        # compress script
├── model_openai.py    # vllm wrapper
├── pred_vllm.py  
├── run.sh             # inference script
├── metrics.py
├── eval.py            # eval results
├── requirements.txt
└── README.md
```


### 使用建议与扩展方向

- 作为论文或项目的压缩基线，对比不同压缩算法在 LongBench 等基准上的效果
- 在本地或服务器上快速复现 LLMLingua 系列方法，并结合 Qwen2.5 等开源模型进行实验
- 根据自己的任务设计新的 prompt 模板和压缩策略，只需复用 `CompressorFactory` 和评测脚本即可
- 将本仓库作为长上下文系统优化的起点，在生产环境中验证压缩对吞吐和成本的收益

如果本仓库对你的研究或项目有帮助，欢迎在 GitHub 上点亮 Star，并在论文或报告中注明引用。也非常期待你的反馈与改进建议。

